\documentclass[11pt, a4paper]{article}
\usepackage[margin=2cm]{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{cancel}

\newcommand\independent{\perp\!\!\!\!\!\!\perp} 

\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\setlength{\parindent}{0pt}

\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}

        \textbf{\LARGE Probabilidade e Estatística}
        \vspace{0.5cm}

        \Large Resumo
        \vspace{1.5cm}

        \textbf{Rafael Rodrigues}
        \vfill
        LEIC \\
        Instituto Superior Técnico \\
        2023/2024
    \end{center}
\end{titlepage}

\tableofcontents

\setcounter{section}{1}

\newpage
\section{Conceitos básicos de probabilidade}


\subsection{Experiência aleatória, espaço de resultados e acontecimentos}

TODO Experiência aleatória, espaço de resultados e acontecimentos

\subsection{Noção de probabilidade. Probabilidade condicionada e lei da probabilidade total}

\subsubsection*{Axiomática de probabilidade}

TODO Axiomática de probabilidade

\subsubsection*{Teoremas decorrentes}

\begin{equation*}
    A \subset  B \Rightarrow P(A) \leq P(B) \ e \ P(B - A) = P(B) - P(A);
\end{equation*}
\begin{equation*}
    P(B - A) = P(B) - P(A \cap B);
\end{equation*}
\begin{equation*}
    P(A \cup B) = P(A) + P(B) - P(A \cap B).
\end{equation*}

\subsubsection*{Probabilidade condicional}

\begin{equation*}
    P(A|B) = \frac{P(A \cap B)}{P(B)} \text{ , se } P(B) > 0
\end{equation*}
\begin{equation*}
    P(\overline{A}|B) = 1 - P(A|B)
\end{equation*}

\subsubsection*{Teorema da probabilidade composta}

\begin{equation*}
    P(A \cap B) = P(A)P(B|A) \vee P(B)P(A|B)
\end{equation*}

\subsubsection*{Teorema da probabilidade total}

Caso os eventos $A_1, ..., A_n$ formem partições do espaço de resultados e B um acontecimento nesse espaço de resultados:

\begin{equation*}
    P(B) = \sum_{i=1}^{n} P(A_i)P(B|A_i)
\end{equation*}

\subsection{Teorema de Bayes}

\begin{equation*}
    P(A_i|B) = \frac{P(A_i \cap B)}{P(B)}
    = \frac{P(A_i) \, P(B|A_i)}{\sum_{j=1}^{n}P(A_j) \, P(B|A_j)}
\end{equation*}

\subsection{Acontecimentos independentes}


\begin{equation*}
    P(A \cap B) = P(A) \times P(B)
\end{equation*}

\newpage
\section{Variáveis aleatórias discretas e contínuas}

\subsection{Definição de variável aleatória. Função de distribuição. Função de massa de probabilidade e função de densidade de probabilidade}

TODO

\subsection{Valor esperado, moda, variância e quantis}

\subsubsection*{Valor esperado de uma variável aleatória}

TODO Valor esperado de uma variável aleatória

\subsubsection*{Valor esperado de uma função de uma variável aleatória}

TODO Valor esperado de uma função de uma variável aleatória

\subsubsection*{Momentos simples e centrais}

TODO Momentos simples e centrais

\subsubsection*{Outros parâmetros: Moda e quantis}

TODO Moda e quantis

\subsection{Distribuições de probabilidade mais utilizadas na modelação de dados}

\subsubsection*{Distribuição Uniforme Discreta}

TODO Distribuição Uniforme Discreta

\subsubsection*{Distribuição Bernoulli}

TODO Distribuição Bernoulli

\subsubsection*{Distribuição Binomial}

TODO Distribuição Binomial

\subsubsection*{Distribuição Geométrica}

\begin{equation*}
    P(X \geq x) = (1-p)^{x-1}
\end{equation*}

Propriedade de falta de memória:
$P(X > i + j | X > j) = P(X > i) \, , \ \forall \ i, j \in \mathbb{N}$

\subsubsection*{Distribuição Poisson}

TODO Distribuição Poisson

\subsubsection*{Distribuição Uniforme Contínua}

\begin{equation*}
    P(X > n) = \int_{n}^{+\infty} f_X(x) \, dx
\end{equation*}
\begin{equation*}
    E(X) = \int_{-\infty}^{+\infty} x \cdot f_X(x) \, dx
\end{equation*}
\begin{equation*}
    E(X^2) = \int_{-\infty}^{+\infty} x^2 \cdot f_X(x) \, dx
\end{equation*}
\begin{equation*}
    V(X) = E(X^2) - E^2(X)
\end{equation*}

\subsubsection*{Distribuição Exponencial}

\begin{equation*}
    P(X > x) = e^{-\lambda x}\ , \ x > 0
\end{equation*}
\begin{equation*}
    P(X \leq x) = 1 - e^{-\lambda x}\ , \ x > 0
\end{equation*}

Propriedade de falta de memória:
$P(X > s + t | X > t) = P(X > s) \, , \ \forall \  s, t \in \mathbb{N}$

\subsubsection*{Distribuição Normal (ou de Gauss)}

\begin{equation*}
    X \backsim N(\mu, \sigma^2)
\end{equation*}
\begin{equation*}
    E(X) = \mu \ \ (\text{média} = \mu)
\end{equation*}
\begin{equation*}
    V(X) = \sigma^2 \ \ (\text{desvio padrão} = \sigma)
\end{equation*}
\begin{equation*}
    P(X \leq x) = P\left(Z \leq \frac{x-\mu}{\sigma}\right) =
    \Phi\left(\frac{x-\mu}{\sigma}\right) \ , \
    Z \backsim N(0,1) \ (\text{Distribuição Normal Reduzida})
\end{equation*}

\newpage
\section{Pares aleatórios}

\subsection{Distribuição conjunta, marginais e condicionais}

\subsubsection*{Função de distribuição conjunta}

TODO Função de distribuição conjunta

\subsubsection*{Funções de distribuição marginais}

TODO Funções de distribuição marginais

\subsubsection*{Distribuição conjunta}

TODO Distribuição conjunta

\subsubsection*{Distribuições marginais}

TODO Distribuições marginais

\subsubsection*{Distribuições condicionais}

TODO Distribuições condicionais

\subsection{Independência}

X e Y são v.a. independentes (X $\independent$ Y) \textbf{sse}:

\begin{equation*}
    P(X = x, Y = y) = P(X = x) \times P(Y = y) \ , \
    \forall(x, y) \in \mathbb{R}^2
\end{equation*}

Caso exista um zero as variáveis \textbf{não são independentes}.

\subsubsection*{Vetores aleatórios discretos e contínuos}

TODO

\subsection*{Valor esperado de uma função de um par aleatório discreto e contínuo}

Para duas v.a. \textbf{dependentes}:

\begin{equation*}
    E(XY) = \sum_{x_i} \sum_{y_j} x_i \, y_j \, f_{X,Y}(x_i, y_j)
\end{equation*}

Para duas v.a. \textbf{independentes}:

\begin{equation*}
    E(XY) = E(X) \, E(Y)
\end{equation*}

\subsection{Covariância}

\begin{equation*}
    cov(X,Y) = E(XY) - E(X) \times E(Y)
\end{equation*}

Se duas v.a. \textbf{são independentes} então a sua \textbf{covariância é nula}.

\subsection{Correlação}

\begin{equation*}
    corr(X,Y) = \frac{cov(X,Y)}{\sqrt{V(X) V(Y)}}
\end{equation*}

\newpage
\section{Complementos das distribuições de probabilidade}

\subsection{Combinações lineares de variáveis aleatórias}

Dada uma v.a. Y resultante da combinação linear de $n$ v.a. $X_1, ..., X_n$ e $n$ constantes $c_1, ..., c_n$, temos:

\begin{equation*}
    Y = \sum_{i=1}^{n} c_i X_i
\end{equation*}

\begin{equation*}
    E(Y) = \sum_{i=1}^{n} c_i E(X_i)
\end{equation*}

Caso $X_1, ..., X_n$ sejam variáveis \textbf{independentes}:
\begin{equation*}
    V(Y) =  \sum_{i=1}^{n} c_i^2 \, V(X_i)
\end{equation*}

Caso contrário:
\begin{equation*}
    V(X_1 \pm X_2) = V(X_1) + V(X_2) \pm 2 \ cov(X_1, X_2)
\end{equation*}

\subsection{Teorema Limite Central}

Para uma sucessão de \textbf{v.a. independentes e identicamente distribuídas} (não normais) tem-se:

\begin{equation*}
    S_n = \sum_{i=1}^{n} X_i
\end{equation*}
\begin{equation*}
    \bar{X} = \frac{S_n}{n} \ \text{(média)}
\end{equation*}
\begin{equation*}
    E(S_n) = n \, E(X_1) = n \, \mu
\end{equation*}
\begin{equation*}
    V(S_n) = n \, V(X_1) = n \, \sigma^2
\end{equation*}

\vspace{20pt}
\section{Estimação pontual}


\subsection{Estatísticas e estimadores}


\subsubsection*{Propriedades dos estimadores}

Seja $T = T(X_1, ..., X_n)$ um estimador do parâmetro $\theta$:

\begin{equation*}
    EQM_\theta (T) = E\left([T - \theta]^2\right)
    = V(T) + \left[E(T) - \theta\right]^2
\end{equation*}

Quando um estimador é \textbf{centrado}, ou seja, $E(T) = \theta$ temos:
\begin{equation*}
    EQM_\theta (T) = V(T)
\end{equation*}


\subsection{Método da máxima verosimilhança}

TODO Método da máxima verosimilhança

\subsubsection*{Distribuição qui-quadrado}

TODO Distribuição qui-quadrado

\subsubsection*{Distribuição t-Student}

TODO Distribuição t-Student

\newpage
\section{Estimação intervalar}

\subsubsection*{Método pivotal}

Dada uma v.a $X$ e uma amostra de tamanho $n$, pretende-se determinar um
intervalo de confiança aproximado a $C\%$
(grau de confiança $\alpha = 1 - \frac{C}{100}$):

\begin{enumerate}
    \item Selecionar  a \textbf{v.a. fulcral} ($Z$) para o parâmetro desconhecido de $X$
    \item Obter os quantis de probabilidade
          \begin{enumerate}
              \item $P(a \leq Z \leq b) = 1 - \alpha$
              \item $\displaystyle P(Z < a) = P(Z > b) = \alpha / 2$
              \item $
                        \begin{array}{l}
                            \displaystyle a = F^{-1}_Z \left(\alpha / 2\right) \\
                            \displaystyle b = F^{-1}_Z \left(1 - \alpha / 2\right)
                        \end{array}$
                    , nas distribuições \textbf{normais e t-Student} temos $a = -b$.
          \end{enumerate}
    \item Inverter a desigualdade $a \leq Z \leq b$, isolando o parâmetro
          desconhecido (ordem crescente)
    \item Concretizar a estimação, substituindo as variáveis pelos valores correspondentes
\end{enumerate}

\subsection{Intervalos de confiança para parâmetros de populações normais}

Se o parâmetro desconhecido é $E(X) = \mu$ então a estimativa pontual é:
\begin{equation*}
    \bar{X} = \frac{1}{n} \sum_{i = 1}^{n} X_i
\end{equation*}

Se o parâmetro desconhecida é $V(X) = \sigma^2$ então a estimativa pontual é:
\begin{equation*}
    S^2 = \frac{1}{n - 1} \sum_{i = 1}^{n} (X_i - \bar{X})^2 \Leftrightarrow
    S^2 = \frac{1}{n - 1} \left[\sum_{i = 1}^{n} X_i^2 - n\bar{X}^2\right]
\end{equation*}

\subsubsection*{Intervalos de confiança para o valor esperado, variância conhecida}

\begin{equation*}
    Z = \frac{\bar{X} - \mu}{\sqrt{\sigma^2 / n}} \thicksim N(0, 1)
\end{equation*}

\subsubsection*{Intervalos de confiança para o valor esperado, variância desconhecida}

\begin{equation*}
    Z = \frac{\bar{X} - \mu}{\sqrt{S^2 / n}} \thicksim t_{(n - 1)}
\end{equation*}


\subsubsection*{Intervalos de confiança para a variância, valor esperado desconhecido}

\begin{equation*}
    Z = \frac{(n - 1) S^2}{\sigma^2} \thicksim \mathcal{X}^2_{(n - 1)}
\end{equation*}

\subsection{Intervalos de confiança para parâmetros de populações de Bernoulli}

\begin{equation*}
    Z = \frac{\bar{X} - p}{\sqrt{\frac{\bar{X} (1 - \bar{X})}{n}}} \thicksim N(0, 1)
\end{equation*}

\newpage
\section{Testes de hipóteses}

\subsection{Testes de hipóteses para parâmetros de populações normais}

\subsection{Testes de hipóteses para a média de uma população normal, com variância desconhecida}

\subsection{Testes de hipóteses para a variância de uma população normal}

\subsection{Testes de hipóteses para parâmetros de populações Bernoulli}

\subsection{Teste de ajustamento do qui-quadrado de Pearson}

\vspace{20pt}
\section{Introdução à regressão linear simples}

\subsection{Modelo de regressão linear simples}

\subsection{Intervalos de confiança e testes de hipóteses para os parâmetros $\beta_0$, $\beta_1$ e $\beta_0 + \beta_1x_0$ do modelo de regressão linear simples}

\subsection{Coeficiente de determinação}


\end{document}