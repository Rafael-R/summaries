\documentclass[twocolumn, 10pt]{article}
\usepackage[a4paper, margin=10pt]{geometry}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tabularray}

\begin{document}

\begin{itemize}[leftmargin=*, itemsep=0pt]
    
    \item Boxplot:
    \begin{itemize}[topsep=0pt]
        \item $Q_{1}=N\times0.25$
        \item $Q_{2}=N\times0.5$
        \item $Q_{3}=N\times0.75$
        \item $IQR=Q_{3}-Q_{1}$
        \item $Bounds=[Q_{1}-1.5 \times IQR, Q_{3}+1.5 \times IQR]$
    \end{itemize}

    \item Pearson = $\displaystyle \frac{\Sigma(y_{1i}-\bar{y}_{1})(y_{2i}-\bar{y}_{2})}{\sqrt[]{\Sigma(y_{1i}-\bar{y}_{1})^2\times(y_{2i}-\bar{y}_{2})^2}}$

    \item Spearman: Assign ranks and apply Pearson formula. \\
    Example: [20, 10, 20, 30, 20] $\rightarrow$ [3, 1, 3, 5, 3]

    \item Normalization:
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item MinMax: $\displaystyle \frac{y_i-min}{max-min}$
        \item Standardization: $\displaystyle \frac{y_i-\mu}{\sigma}$
    \end{itemize}

    \item Binarization: 
    \begin{itemize}[topsep=0pt]
        \item Range\ (equal\ width): Depends on variable range \\
        Example: $y\in[-1,1]: [0.2, -0.1, 0.6] \rightarrow [1, 0, 1]$
        \item Frequency\ (equal\ depth): Depends on variable mean \\
        Example: $\bar{y}=25: [10, 40, 30, 20] \rightarrow [0, 1, 1, 0]$
    \end{itemize} 

    \item Confusion Matrix:
    \begin{tblr}{ 
        columns = {1.4em,c},
        column{1,6} = {0.4em},
        rowsep = 1pt,
        hline{3,6} = {1-5}{black}, hline{1,4-5} = {3-5}{black},
        vline{3,6} = {black}, vline{1,4-5} = {3-5}{black},
        cell{4}{4} = {green7}, cell{2}{7} = {green7},
        cell{3,5}{3,5} = {red7}, cell{3}{7} = {red7},
        cell{4}{3,5} = {green9}, cell{4}{7} = {green9},
        cell{3,5}{4} = {red9}, cell{5}{7} = {red9},
    }
        & & \SetCell[c=3]{c} True & & & & B \\
        & & A & B & C & & TP \\
        \SetCell[r=3]{c}\rotatebox[origin=c]{90}{Pred} & A & TA & FA & FA & & TN \\
        & B & FB & TB & FB & & FP \\
        & C & FC & FC & TC & & FN \\
    \end{tblr}

    \item Metrics:
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item Accuracy = $\displaystyle \frac{TP+TN}{total}$
        \item Error rate = $\displaystyle 1-Accuracy=\frac{FP+FN}{total}$
        \item Recall = $\displaystyle \frac{TP}{TP+FN}$ (Sensitivity)
        \item Fallout = $\displaystyle \frac{TN}{TN+FP}$ (Specificity)
        \item Precision = $\displaystyle \frac{TP}{TP+FP}$
        \item F$_1$ = $\displaystyle \frac{TP}{TP+\frac{1}{2}(FP+FN)}$
    \end{itemize}

    \item Error: \\
    Sum of Squares Error: $\sum(z-\hat{z})^2$ \\
    Root Maen Squared Error: $\sqrt[]{\frac{1}{n}SSE}$ \\
    Mean Absolute (Percentage) Error: $\displaystyle \frac{1}{n}\sum|\frac{z-\hat{z}}{(z)}|$

    \item Information Gain: $IG(class|y_i)=E(class)-E(class|y_i)$

    \item Entropy: $\displaystyle E(y)=\sum_{i=1}^{k} \frac{|y_i|}{|y|}
                                       \sum_{j=1}^{n} -P(y_{ij})\log(P(y_{ij}))$

    \item Decision trees:
    \begin{enumerate}[topsep=0pt, itemsep=0pt]
        \item Choose feature with highest IG.
        \item Split dataset by that feature, create leaves if necessary.
        \item Repeat until unable to proceed.
    \end{enumerate}
    Prune: (Given a twig)
    \begin{enumerate}[topsep=0pt, itemsep=0pt]
        \item Count it's leaves labels. \; Example: $\#A=5, \ \#B=6$
        \item Remove it's leaves.
        \item Relabel twig as a leaf. \; Example: $B(6/11), \ \ \#B>\#A$ 
    \end{enumerate}

    \item Vector Norm: \\[2pt]
    $\displaystyle\left\lVert x\right\rVert _p=\sqrt[p]{\sum_{i=1}^n\left\lvert x_i\right\rvert ^p}$ \;\;\;\;\; $\displaystyle\left\lVert x\right\rVert _\infty=max \left\lvert x_i\right\rvert  $

    \newpage
    \item Matrix Multiplication: \\[3pt]
    $\begin{bmatrix}
        \ldots & n \\
        m & \ldots \\
    \end{bmatrix} \cdot
    \begin{bmatrix}
        \ldots & l \\
        n & \ldots \\
    \end{bmatrix} =
    \begin{bmatrix}
        \ldots  & l \\
        m  & \ldots \\
    \end{bmatrix}$

    \item Gaussian Distribution:
    \begin{itemize}[topsep=0pt]
        \item Variance: $\displaystyle var=\frac{\sum(y_i-\mu)^2}{n(-1)}$
        \item Standard Deviation: $\displaystyle \sigma=\sqrt[]{var}$
        \item $\displaystyle  P(y|\mu,\sigma)=\frac{1}{\sqrt[]{2\pi}\cdot\sigma}\cdot\exp\left(-\frac{1}{2}\left(\frac{y-\mu}{\sigma}\right)^2\right)$
    \end{itemize}

    \item Gaussian Mixture:
    \begin{itemize}[topsep=0pt]
        \item Covariance: $\displaystyle cov(y_1,y_2)=\frac{\sum(y_{1i}-\mu_1)(y_{2i}-\mu_2)}{n(-1)}$
        \item Covariance Matrix: $\Sigma(y_1,y_2)=
        \begin{bmatrix}
            var(y_1) & cov(y_1,y_2) \\
            cov(y_1,y_2) & var(y_2) \\
        \end{bmatrix}$
        \item $det\left\lvert\Sigma\right\rvert=det
        \begin{vmatrix}
            a & b \\
            c & d \\
        \end{vmatrix}
        = ad-bc$
        \item $\displaystyle P(y|\mu,\Sigma)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}\cdot exp\left(-\frac{1}{2}(y-\mu)^T\Sigma^{-1}(y-\mu)\right)$
    \end{itemize}

    \item Naive Bayes: 
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item MAP: $\displaystyle P(C|x)=\frac{P(C)P(x|C)}{P(x)}$
        \item ML: $\displaystyle P(C|x)=P(x|C)$
    \end{itemize}
    \begin{enumerate}[topsep=0pt, itemsep=0pt]
        \item Calculate $P(C)$ for each class.
        \item Calculate $P(y|C)$ for each variable for each class.
        \item Calculate likelihood: $P(x|C)=P(y_1|C)\times\ldots\times P(y_d|C)$
    \end{enumerate}
    
    \item K-Nearest Neighbors:
    \begin{enumerate}[topsep=0pt, itemsep=0pt]
        \item Distances: (for n variables)
        \begin{itemize}[topsep=0pt]
            \item Manhattan: $\displaystyle \sum |y_{1i}-y_{2i}|$
            \item Euclidean: $\displaystyle \sqrt[]{\sum (y_{1i}-y_{2i})^2}$
            \item Cosine: $\displaystyle \frac{\sum y_{1i}\ y_{2i}}
            {\sqrt[]{\sum y_{1i}^2}\ \sqrt[]{\sum y_{2i}^2}}$
            \item Hamming: \#Differences
        \end{itemize}
        \item If weighted, for each variable multiply by weight.
        \item Choose K nearest neighbors.
        \item Classify using mean if variable is numeric, \\
        or mode if it is categoric.
    \end{enumerate}

    \item Regressions: \; $\hat{z}=Xw=(w^TX^T)^T$
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item Linear: $y=w_0+w_1x$
        \item Polynomial: $w=(X^TX)^{-1}X^Tz$
        \item Ridge: $w=(X^TX+\lambda I)^{-1}X^Tz$
    \end{itemize}

    \item Perceptron: \\
    $\hat{z}=a(w^Tx)$, \ $a  \leftarrow$ activation function \\[2pt]
    Gradient Descent: \\
    $w^{new}=w^{old}+\Delta w$ \; where \; 
    $\displaystyle \Delta w=-\eta\frac{\partial E}{\partial w}$

    \item Neural Networks (MLP):
    \begin{itemize}[topsep=0pt]
        \item Forward: $x^{[0]}\rightarrow z^{[1]}=w^{[1]}x^{[0]}+b^{[1]}\rightarrow x^{[1]}=a\left(z^{[1]}\right) \rightarrow \ldots \rightarrow z^{[i]}=w^{[i]}x^{[i-1]}+b^{[i]} \rightarrow x^{[i]}=a\left(z^{[i]}\right) \rightarrow E $
        \item Backward:
        \begin{itemize}[topsep=0pt, itemsep=3pt]
            \item $\displaystyle \delta^{[last]}=\frac{\partial E}{\partial x^{[last]}}\circ \frac{\partial x^{[last]}}{\partial z^{[last]}}$
            \item $\displaystyle \delta^{[i]}=\left(w^{[i+1]}\right)^T\cdot\delta^{[i+1]}\circ\frac{\partial x^{[i]}}{\partial z^{[i]}}$
            \item $\displaystyle w^{[i]'}=w^{[i]}-\eta\frac{\partial E}{\partial w^{[i]}}$ \;\;\;\, $*\ \displaystyle \frac{\partial E}{\partial w^{[i]}}=\delta^{[i]}\cdot \left(x^{[i-1]}\right)^T$
            \item $\displaystyle b^{[i]'}=b^{[i]}-\eta\frac{\partial E}{\partial b^{[i]}}$ \;\;\;\;\;\;\;\  $*\ \displaystyle \frac{\partial E}{\partial b^{[i]}}=\delta^{[i]}$
        \end{itemize}
        \newpage
        \item Derivatives: \\[4pt]
        \begin{tblr}{
            colspec = { X[c,m]X[c,m]X[c,m] },
            hlines = {black},
            vlines = {black},
        }
            Name & {Error \\ function} & $\displaystyle \frac{\partial E}{\partial x^{[i]}}$ \\
            {Squared \\ Error} & $\displaystyle \frac{1}{2}\left(x^{[i]}-t\right)^2$ & $x^{[i]}-t$ \\
            {Cross-\\-entropy} & $\displaystyle -\sum_{i=1}^{n}t_{i}\log\left(x_{i}^{[i]}\right)$ & $\displaystyle -\frac{t}{x^{[i]}}+\frac{1-t}{1-x^{[i]}}$ \\
        \end{tblr} \\[4pt]
        \begin{tblr}{
            colspec = { X[c,m]X[c,m]X[c,m] },
            hlines = {black},
            vlines = {black},
        }
            Name & Activation function & $\displaystyle \frac{\partial x^{[i]}}{\partial z^{[i]}}$ \\
            {Sigmoid \\ $\sigma\left(x\right)$} & $\displaystyle \frac{1}{1+e^{-x}}$ & $x^{[i]}\left(1-x^{[i]}\right)$ \\
            {ArcTan \\ $\arctan(x)$} & {$\arctan(x)$ \\ \mbox{or } $\tan^{-1}(x)$} & $\displaystyle \frac{1}{\left(x^{[i]}\right)^2+1}$ \\
            Hyperbolic tangent & $\displaystyle \frac{e^x-e^{-x}}{e^x+e^{-x}}$ & $1-\left(x^{[i]}\right)^2$ \\
            ReLU & {$0\mbox{ if }x<0$ \\ $x\mbox{ if }x\geq 0$} & {$0\mbox{ if }x^{[i]}<0$ \\ $1\mbox{ if }x^{[i]}\geq0$} \\
            Softmax & $\displaystyle \frac{e^{x_i}}{\sum_{j=1}^{n}e^{x_j}}$ & $x^{[i]}\left(1-x^{[i]}\right)$ \\
            Sign & {$1\mbox{ if }x\geq 0$ \\ $0\mbox{ if }x<0$} & \\
        \end{tblr} \\[4pt]
        NOTE:
        \begin{itemize}
            \item When cross-entropy and softmax are combined: \\[2pt]
            $\displaystyle \delta^{[last]}=\frac{\partial E}{\partial z^{[last]}}=x^{[last]}-t$
        \end{itemize}
    \end{itemize}

    \item K-Means:
    \begin{enumerate}[topsep=0pt, itemsep=0pt]
        \item Assign each point to a cluster.
        \item Update centroids: centroid$_{new}=\mu$\ of cluster's points
        \item Repeat until centroids don't change.
    \end{enumerate}

    \item EM:
    \begin{itemize}[topsep=0pt]
        \item Initializaion: Initial mixture parameters
        \item Expectation (E-step): \\[2pt]
        Calculate weights for each datapoint x$_i$ for each cluster c$_k$: 
        $\displaystyle \gamma_{ki}=\frac{\mathcal{N}(x_i|\mu_k,\Sigma_k)\cdot\pi_k}{\sum_{j = 1}^{k}\mathcal{N}(x_{i}|\mu_{j},\Sigma_{j})\cdot\pi_{j}}$
        \item Maximization (M-step): \\[2pt]
        Update parameters for each cluster: (for n observations)
        \begin{itemize}
            \item $\displaystyle N_{k}={\sum_{i=1}^{n}\gamma_{ki}}$
            \item $\displaystyle \mu_{k}={\frac{1}{N_{k}}\sum_{i=1}^{n}\gamma_{ki}\cdot x_{i}}$
            \item $\displaystyle \Sigma_{k} = \frac{1}{N_{k}}\sum_{i=1}^{n}\gamma_{ki} \cdot (x_{i} - \mu_{k}) \cdot (x_{i} - \mu_{k})^T$
            \item $\displaystyle \pi_{k}={\frac{N_{k}}{N}}$
        \end{itemize}
    \end{itemize}

    \item Sillhouette: $\in$ [-1,1] (the closer to 1 the better)
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item For an observation x$_i$:
        \begin{itemize}[topsep=0pt, itemsep=0pt]
            \item $a$ = average distance of x$_i$ to the points in it's cluster
            \item $b$ = average distance of x$_i$ to points in closest cluster
            \item $\displaystyle s_{observation}=\frac{b-a}{max(a,b)}$
        \end{itemize}
        \item For a cluster: \\[2pt]
        Average of the cluster's observations silhouettes
        \item For the solution: \\[2pt]
        Average of the clusters silhouettes
    \end{itemize}

    \newpage
    \NewColumnType{C}{>{$}Q[c,m]<{$}}
    \item More Derivatives: \\[4pt]
    \begin{tblr}{
        colspec = {CC},
        row{1} = {c},
        hlines = {black},
        vlines = {black},
    }
        \mbox{Function} & \mbox{Derivative} \\
        u^n & n\ u^{n-1}\ u' \\
        uv & u'v+v'u \\
        e^u & e^{u}u' \\
    \end{tblr} \\[4pt]

    \item PCA: \\[2pt]
    $C u_i=(\lambda_iI) u_i$
    \begin{itemize}[topsep=0pt]
        \item $C$: Covariance Matrix
        \item $I$: Identity Matrix
        \item Eigenvector ($u$): $(C-\lambda I)u=\vec{0}$
        \item Eigenvalue ($\lambda$): $det\left\lvert C-\lambda I\right\rvert=0$
        \item Higher eigenvalue means more variation
    \end{itemize}
    Example:
    \begin{flalign*}
        \left(\begin{bmatrix}
            1.333 & 0.667 \\
            0.667 & 1.667 \\
        \end{bmatrix} - 
        \begin{bmatrix}
            2.187 & 0 \\
            0 & 2.187 \\
        \end{bmatrix}\right)
        \begin{bmatrix}
            x_1 \\
            x_2 \\
        \end{bmatrix} = 
        \begin{bmatrix}
            0 \\
            0 \\
        \end{bmatrix} \Leftrightarrow \\
        \begin{bmatrix}
            0.854 & 0.667 \\
            0.667 & 0.52  \\
        \end{bmatrix}
        \begin{bmatrix}
            x_1 \\
            x_2 \\
        \end{bmatrix} = 
        \begin{bmatrix}
            0 \\
            0 \\
        \end{bmatrix} \Leftrightarrow \\
    \end{flalign*}

    \item Model complexity:
    \begin{itemize}[topsep=0pt]
        \item Perceptron with $d$ inputs: $d+1$
        \item Tree with $d$ features que tomam $n$ valores: $\displaystyle n^d$
        \item MLP: estimated by the number of weights
        \item Bayesian Classifier: estimated by the number of parameters
        \begin{itemize}[topsep=0pt, itemsep=0pt]
            \item With $c$ classes: $c-1\ priors$
            \item With $d$ dimension: $\displaystyle (d\ averages+\frac{d(d+1)}{2}\Sigma)\times c$
        \end{itemize}
    \end{itemize}

\end{itemize}

\end{document}