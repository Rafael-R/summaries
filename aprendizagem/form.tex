\documentclass[twocolumn, 10pt]{article}
\usepackage[a4paper, margin=10pt]{geometry}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tabularray}
\usepackage{mathtools}

\newenvironment{nsflalign*}
    {\setlength{\abovedisplayskip}{2pt}\setlength{\belowdisplayskip}{0pt}%
        \csname flalign*\endcsname}
    {\csname endflalign*\endcsname\ignorespacesafterend}

\newcommand{\twopartdef}[2]
{
	\left\{
		\begin{array}{ll}
			#1 \\
			#2 
		\end{array}
	\right.
}

\NewColumnType{C}{>{$}Q[c,m]<{$}}

\begin{document}

\begin{itemize}[leftmargin=*, itemsep=0pt]
    
    \item Boxplot:
    \begin{itemize}[topsep=0pt]
        \item $Q_{1}=N\times0.25$
        \item $Q_{2}=N\times0.5$
        \item $Q_{3}=N\times0.75$
        \item $IQR=Q_{3}-Q_{1}$
        \item $Bounds=[Q_{1}-1.5 \times IQR, Q_{3}+1.5 \times IQR]$
    \end{itemize}

    \item Pearson = $\displaystyle \frac{\Sigma(y_{1i}-\bar{y}_{1})(y_{2i}-\bar{y}_{2})}{\sqrt[]{\Sigma(y_{1i}-\bar{y}_{1})^2\times(y_{2i}-\bar{y}_{2})^2}}$

    \item Spearman: Assign ranks and apply Pearson formula. \\
    Example: [20, 10, 20, 30, 20] $\rightarrow$ [3, 1, 3, 5, 3]

    \item Normalization:
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item MinMax: $\displaystyle \frac{y_i-min}{max-min}$
        \item Standardization: $\displaystyle \frac{y_i-\mu}{\sigma}$
    \end{itemize}

    \item Binarization: 
    \begin{itemize}[topsep=0pt]
        \item Range\ (equal\ width): Depends on variable range \\
        Example: $y\in[-1,1]: [0.2, -0.1, 0.6] \rightarrow [1, 0, 1]$
        \item Frequency\ (equal\ depth): Depends on variable mean \\
        Example: $\bar{y}=25: [10, 40, 30, 20] \rightarrow [0, 1, 1, 0]$
    \end{itemize} 

    \item Confusion Matrix:
    \begin{tblr}{ 
        columns = {1.4em,c},
        column{1,6} = {0.4em},
        rowsep = 1pt,
        hline{3,6} = {1-5}{black}, hline{1,4-5} = {3-5}{black},
        vline{3,6} = {black}, vline{1,4-5} = {3-5}{black},
        cell{4}{4} = {green7}, cell{2}{7} = {green7},
        cell{3,5}{3,5} = {red7}, cell{3}{7} = {red7},
        cell{4}{3,5} = {green9}, cell{4}{7} = {green9},
        cell{3,5}{4} = {red9}, cell{5}{7} = {red9},
    }
        & & \SetCell[c=3]{c} True & & & & B \\
        & & A & B & C & & TP \\
        \SetCell[r=3]{c}\rotatebox[origin=c]{90}{Pred} & A & TA & FA & FA & & TN \\
        & B & FB & TB & FB & & FP \\
        & C & FC & FC & TC & & FN \\
    \end{tblr}

    \item Metrics:
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item Accuracy = $\displaystyle \frac{TP+TN}{total}$
        \item Error rate = $\displaystyle 1-Accuracy=\frac{FP+FN}{total}$
        \item Recall = $\displaystyle \frac{TP}{TP+FN}$ (Sensitivity)
        \item Fallout = $\displaystyle \frac{TN}{TN+FP}$ (Specificity)
        \item Precision = $\displaystyle \frac{TP}{TP+FP}$
        \item F$_1$ = $\displaystyle \frac{TP}{TP+\frac{1}{2}(FP+FN)}$
    \end{itemize}

    \item Error: \\
    Sum of Squares Error: $\sum(z-\hat{z})^2$ \\
    Root Maen Squared Error: $\sqrt[]{\frac{1}{n}SSE}$ \\
    Mean Absolute (Percentage) Error: $\displaystyle \frac{1}{n}\sum|\frac{z-\hat{z}}{(z)}|$

    \item Information Gain: $IG(class|y_i)=E(class)-E(class|y_i)$

    \item Entropy: $\displaystyle E(y)=\sum_{i=1}^{k} \frac{|y_i|}{|y|}
                                       \sum_{j=1}^{n} -P(y_{ij})\log(P(y_{ij}))$

    \item Decision trees:
    \begin{enumerate}[topsep=0pt, itemsep=0pt]
        \item Choose feature with highest IG.
        \item Split dataset by that feature, create leaves if necessary.
        \item Repeat until unable to proceed.
    \end{enumerate}
    Prune: (Given a twig)
    \begin{enumerate}[topsep=0pt, itemsep=0pt]
        \item Count it's leaves labels. \; Example: $\#A=5, \ \#B=6$
        \item Remove it's leaves.
        \item Relabel twig as a leaf. \; Example: $B(6/11), \ \ \#B>\#A$ 
    \end{enumerate}

    \item Vector Norm: \\[2pt]
    $\displaystyle\left\lVert x\right\rVert _p=\sqrt[p]{\sum_{i=1}^n\left\lvert x_i\right\rvert ^p}$ \;\;\;\;\; $\displaystyle\left\lVert x\right\rVert _\infty=max \left\lvert x_i\right\rvert  $

    \newpage
    \item Matrix Multiplication: \\[3pt]
    $\begin{bmatrix}
        \ldots & n \\
        m & \ldots \\
    \end{bmatrix} \cdot
    \begin{bmatrix}
        \ldots & l \\
        n & \ldots \\
    \end{bmatrix} =
    \begin{bmatrix}
        \ldots  & l \\
        m  & \ldots \\
    \end{bmatrix}$

    \item Gaussian Distribution:
    \begin{itemize}[topsep=0pt]
        \item Variance: $\displaystyle var=\frac{\sum(y_i-\mu)^2}{n(-1)}$
        \item Standard Deviation: $\displaystyle \sigma=\sqrt[]{var}$
        \item $\displaystyle  P(y|\mu,\sigma)=\frac{1}{\sqrt[]{2\pi}\cdot\sigma}\cdot\exp\left(-\frac{1}{2}\left(\frac{y-\mu}{\sigma}\right)^2\right)$
    \end{itemize}

    \item Gaussian Mixture:
    \begin{itemize}[topsep=0pt]
        \item Covariance: $\displaystyle cov(y_1,y_2)=\frac{\sum(y_{1i}-\mu_1)(y_{2i}-\mu_2)}{n(-1)}$
        \item Covariance Matrix: $\Sigma(y_1,y_2)=
        \begin{bmatrix}
            var(y_1) & cov(y_1,y_2) \\
            cov(y_1,y_2) & var(y_2) \\
        \end{bmatrix}$
        \item $det\left\lvert\Sigma\right\rvert=det
        \begin{vmatrix}
            a & b \\
            c & d \\
        \end{vmatrix}
        = ad-bc$
        \item $\displaystyle P(y|\mu,\Sigma)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}\cdot exp\left(-\frac{1}{2}(y-\mu)^T\Sigma^{-1}(y-\mu)\right)$
    \end{itemize}

    \item Naive Bayes: 
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item MAP: $\displaystyle P(C|x)=\frac{P(C)P(x|C)}{P(x)}$
        \item ML: $\displaystyle P(C|x)=P(x|C)$
    \end{itemize}
    \begin{enumerate}[topsep=0pt, itemsep=0pt]
        \item Calculate $P(C)$ for each class.
        \item Calculate $P(y|C)$ for each variable for each class.
        \item Calculate likelihood: $P(x|C)=P(y_1|C)\times\ldots\times P(y_d|C)$
    \end{enumerate}
    
    \item K-Nearest Neighbors:
    \begin{enumerate}[topsep=0pt, itemsep=0pt]
        \item Distances: (for n variables)
        \begin{itemize}[topsep=0pt]
            \item Manhattan: $\displaystyle \sum |y_{1i}-y_{2i}|$
            \item Euclidean: $\displaystyle \sqrt[]{\sum (y_{1i}-y_{2i})^2}$
            \item Cosine: $\displaystyle \frac{\sum y_{1i}\ y_{2i}}
            {\sqrt[]{\sum y_{1i}^2}\ \sqrt[]{\sum y_{2i}^2}}$
            \item Hamming: \#Differences
        \end{itemize}
        \item If weighted, for each variable multiply by weight.
        \item Choose K nearest neighbors.
        \item Classify using mean if variable is numeric, \\
        or mode if it is categoric.
    \end{enumerate}

    \item Regressions: \; $\hat{z}=Xw=(w^TX^T)^T$
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item Linear: $y=w_0+w_1x$
        \item Polynomial: $w=(X^TX)^{-1}X^Tz$
        \item Ridge: $w=(X^TX+\lambda I)^{-1}X^Tz$
    \end{itemize}

    \item Perceptron: \\
    $\hat{z}=a(w^Tx)$, \ $a  \leftarrow$ activation function \\[2pt]
    Gradient Descent: \\
    $w^{new}=w^{old}+\Delta w$ \; where \; 
    $\displaystyle \Delta w=-\eta\frac{\partial E}{\partial w}$

    \item Neural Networks (MLP):
    \begin{itemize}[topsep=0pt]
        \item Forward: $x^{[0]}\rightarrow z^{[1]}=w^{[1]}x^{[0]}+b^{[1]}\rightarrow x^{[1]}=a\left(z^{[1]}\right) \rightarrow \ldots \rightarrow z^{[i]}=w^{[i]}x^{[i-1]}+b^{[i]} \rightarrow x^{[i]}=a\left(z^{[i]}\right)$

        \item Backward:
        \begin{itemize}[topsep=0pt, itemsep=3pt]
            \item $\displaystyle \delta^{[last]}=\frac{\partial E}{\partial x^{[last]}}\circ \frac{\partial x^{[last]}}{\partial z^{[last]}}$
            \item $\displaystyle \delta^{[i]}=\left(w^{[i+1]}\right)^T\cdot\delta^{[i+1]}\circ\frac{\partial x^{[i]}}{\partial z^{[i]}}$
            \item $\displaystyle w^{[i]'}=w^{[i]}-\eta\cdot\frac{\partial E}{\partial w^{[i]}}$
            \item $\displaystyle b^{[i]'}=b^{[i]}-\eta\cdot\frac{\partial E}{\partial b^{[i]}}$
        \end{itemize}
        \newpage

        \item Multiple observations:
        \begin{enumerate}
            \item Apply forward propagation for each observation.
            \item Calculate $\delta_i^{[l]}$ for each observation.
            \item $\displaystyle \frac{\partial E}{\partial w^{[l]}} = 
                   \sum_{i=1}^{n} \delta_{i}^{[l]}\cdot\left(x_i^{[l-1]}\right)^T$
            \item $\displaystyle \frac{\partial E}{\partial b^{[l]}} = 
                   \sum_{i=1}^{n} \delta_{i}^{[l]}$
        \end{enumerate}
        
        \item Derivatives: \\[4pt]
        \begin{tblr}{
            colspec = { X[c,m]X[c,m]X[c,m] },
            hlines = {black},
            vlines = {black},
        }
            Name & {Error \\ function} & $\displaystyle \frac{\partial E}{\partial x^{[i]}}$ \\
            {Squared \\ Error} & $\displaystyle \frac{1}{2}\left(x^{[i]}-t\right)^2$ & $x^{[i]}-t$ \\
            {Cross-\\-entropy} & $\displaystyle -\sum_{i=1}^{n}t_{i}\log\left(x_{i}^{[i]}\right)$ & $\displaystyle -\frac{t}{x^{[i]}}+\frac{1-t}{1-x^{[i]}}$ \\
        \end{tblr} \\[4pt]
        \begin{tblr}{
            colspec = { X[c,m]X[c,m]X[c,m] },
            hlines = {black},
            vlines = {black},
        }
            Name & Activation function & $\displaystyle \frac{\partial x^{[i]}}{\partial z^{[i]}}$ \\
            {Sigmoid \\ $\sigma\left(x\right)$} & $\displaystyle \frac{1}{1+e^{-x}}$ & $x^{[i]}\left(1-x^{[i]}\right)$ \\
            {ArcTan \\ $\arctan(x)$} & {$\arctan(x)$ \\ \mbox{or } $\tan^{-1}(x)$} & $\displaystyle \frac{1}{\left(x^{[i]}\right)^2+1}$ \\
            Hyperbolic tangent & $\displaystyle \frac{e^x-e^{-x}}{e^x+e^{-x}}$ & $1-\left(x^{[i]}\right)^2$ \\
            ReLU & {$0\mbox{ if }x<0$ \\ $x\mbox{ if }x\geq 0$} & {$0\mbox{ if }x^{[i]}<0$ \\ $1\mbox{ if }x^{[i]}\geq0$} \\
            Softmax & $\displaystyle \frac{e^{x_i}}{\sum_{j=1}^{n}e^{x_j}}$ & $x^{[i]}\left(1-x^{[i]}\right)$ \\
            Sign & {$1\mbox{ if }x\geq 0$ \\ $0\mbox{ if }x<0$} & \\
        \end{tblr} \\[4pt]
        NOTE:
        \begin{itemize}
            \item When cross-entropy and softmax are combined: \\[2pt]
            $\displaystyle \delta^{[last]}=\frac{\partial E}{\partial z^{[last]}}=x^{[last]}-t$
        \end{itemize}
    \end{itemize}

    \item More Derivatives: \\[4pt]
    \begin{tblr}{
        colspec = {CC},
        row{1} = {c},
        hlines = {black},
        vlines = {black},
    }
        \mbox{Function} & \mbox{Derivative} \\
        x\pm y & x'\pm y'\\
        xy & x'y+y'x \\
        \displaystyle\frac{x}{y} & \displaystyle\frac{xy'+y'x}{y^2} \\
    \end{tblr}
    \begin{tblr}{
        colspec = {CC},
        row{1} = {c},
        hlines = {black},
        vlines = {black},
    }
        \mbox{Function} & \mbox{Derivative} \\
        f(x)^a & af(x)^{a-1}f'(x) \\
        a^{f(x)} & a^{f(x)}f'(x)\ln a \\
        \log_a f(x) & \displaystyle\frac{f'(x)}{f(x)\ln a} \\
    \end{tblr}

    \item K-Means:
    \begin{enumerate}[topsep=0pt, itemsep=0pt]
        \item Assign each point to a cluster.
        \item Update centroids: centroid$_{new}=\mu$\ of cluster's points
        \item Repeat until centroids don't change.
    \end{enumerate}

    \item Sillhouette: $\in$ [-1,1] (the closer to 1 the better)
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item For an observation x$_i$:
        \begin{itemize}[topsep=0pt, itemsep=0pt]
            \item $a$ = average distance of x$_i$ to the points in it's cluster
            \item $b$ = average distance of x$_i$ to points in closest cluster
            \item $\displaystyle s_{observation}=\frac{b-a}{max(a,b)}$
        \end{itemize}
        \item For a cluster: \\[2pt]
        Average of the cluster's observations silhouettes
        \item For the solution: \\[2pt]
        Average of the clusters silhouettes
    \end{itemize}

    \newpage
    \item EM:
    \begin{itemize}[topsep=0pt]
        \item Initializaion: Initial mixture parameters
        \item Expectation (E-step): \\[2pt]
        Calculate weights for each datapoint x$_i$ for each cluster c$_k$: 
        $\displaystyle \gamma_{ki}=\frac{\mathcal{N}(x_i|\mu_k,\Sigma_k)\cdot\pi_k}{\sum_{j = 1}^{k}\mathcal{N}(x_{i}|\mu_{j},\Sigma_{j})\cdot\pi_{j}}$
        \item Maximization (M-step): \\[2pt]
        Update parameters for each cluster: (for n observations)
        \begin{itemize}
            \item $\displaystyle N_{k}={\sum_{i=1}^{n}\gamma_{ki}}$
            \item $\displaystyle \mu_{k}={\frac{1}{N_{k}}\sum_{i=1}^{n}\gamma_{ki}\cdot x_{i}}$
            \item $\displaystyle \Sigma_{k} = \frac{1}{N_{k}}\sum_{i=1}^{n}\gamma_{ki} \cdot (x_{i} - \mu_{k}) \cdot (x_{i} - \mu_{k})^T$
            \item $\displaystyle \pi_{k}={\frac{N_{k}}{N}}$
        \end{itemize}
    \end{itemize}

    \item Quadratic Formula:
    $\displaystyle ax^2+bx+c=0 \;\Rightarrow\; x=\frac{-b\pm\sqrt[]{b^2-4ac}}{2a}$

    \item PCA: \; $\Sigma u=(\lambda I) u$
    \begin{itemize}[topsep=0pt]
        \item $\Sigma$: Covariance Matrix
        \item $I$: Identity Matrix
        \item Eigenvalue ($\lambda$):\;
            $det\left\lvert\Sigma - \lambda I\right\rvert = 0$ \; or \;
            $\displaystyle \lambda = \Sigma u \circ \frac{1}{u}$ \\
        Example:
        \begin{nsflalign*}
            \lambda =&
            \left(\begin{bmatrix} 2.917 & 2.667 \\ 2.667 & 2.667 \end{bmatrix}
            \begin{bmatrix} -0.690 \\ 0.723 \end{bmatrix}\right) \circ
            \begin{bmatrix} -0.690^{-1} \\ 0.723^{-1} \end{bmatrix} \\ =&
            \begin{bmatrix} -0.084 \\ 0.088 \end{bmatrix} \circ
            \begin{bmatrix} -0.690^{-1} \\ 0.723^{-1} \end{bmatrix} \\ =&
            \begin{bmatrix} 1.122 \\ 1.122 \end{bmatrix}
        \end{nsflalign*}
        \item Eigenvector ($u$):\;
            $(\Sigma-\lambda I)u=\vec{0}$ \\
        Example:
        \begin{nsflalign*}
            u \Leftrightarrow&
            \left(\begin{bmatrix} 1.333 & 0.667 \\ 0.667 & 1.667 \end{bmatrix} - 
            \begin{bmatrix} 2.187 & 0 \\ 0 & 2.187 \end{bmatrix}\right)
            \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 
            \begin{bmatrix} 0 \\ 0 \end{bmatrix} \\\Leftrightarrow&
            \left[\begin{array}{cc|c}
                -0.854 & 0.667 & 0 \\ 
                0.667 & -0.52 & 0 
            \end{array}\right] \Leftrightarrow 
            \begin{array}{l}
                L_1\times -0.854^{-1} \\
                L_2\times 0.667^{-1}
            \end{array} \\\Leftrightarrow&
            \left[\begin{array}{cc|c} 
                1 & -0.781 & 0 \\ 
                1 & -0.781 & 0
            \end{array}\right] \Leftrightarrow
            \begin{array}{l}
                L_2 - L_1
            \end{array} \\\Leftrightarrow&
            \left[\begin{array}{cc|c} 
                1 & -0.781 & 0 \\ 
                0 & 0 & 0
            \end{array}\right] \Leftrightarrow
            x_1 = 0.781x_2 \\=&
            \begin{bmatrix}
                0.781x \\ x
            \end{bmatrix}
        \end{nsflalign*}
        \item Projecting (bivariate to univariate):
        \begin{enumerate}[topsep=0pt, itemsep=0pt]
            \item Choose highest $\lambda$ (higher $\lambda$ means more variation)
            \item Calculate eigenvector $(u)$
            \item Apply formula: $\phi = u^TX^T$ 
        \end{enumerate}
        
    \end{itemize}

    \item Model complexity:
    \begin{itemize}[topsep=0pt]
        \item Perceptron with $d$ inputs: $d+1$
        \item Tree with $d$ features que tomam $n$ valores: $\displaystyle n^d$
        \item MLP: estimated by the number of weights
        \item Bayesian Classifier: estimated by the number of parameters
        \begin{itemize}[topsep=0pt, itemsep=0pt]
            \item With $c$ classes: $c-1\ priors$
            \item With $d$ dimension: $\displaystyle (d\ averages+\frac{d(d+1)}{2}\Sigma)\times c$
        \end{itemize}
    \end{itemize}

\end{itemize}

\end{document}