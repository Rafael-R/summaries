\documentclass[twocolumn, 10pt]{article}
\usepackage[a4paper, margin=10pt]{geometry}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tabularray}

\begin{document}

\begin{itemize}[leftmargin=*, itemsep=0pt]
    
    \item Boxplot:
    \begin{itemize}[topsep=0pt]
        \item $Q_{1}=N\times0.25$
        \item $Q_{2}=N\times0.5$
        \item $Q_{3}=N\times0.75$
        \item $IQR=Q_{3}-Q_{1}$
        \item $Bounds=[Q_{1}-1.5 \times IQR, Q_{3}+1.5 \times IQR]$
    \end{itemize}

    \item Pearson = $\displaystyle \frac{\Sigma(y_{1i}-\bar{y}_{1})(y_{2i}-\bar{y}_{2})}{\sqrt[]{\Sigma(y_{1i}-\bar{y}_{1})^2\times(y_{2i}-\bar{y}_{2})^2}}$

    \item Spearman: Assign ranks and apply Pearson formula. \\
    Example: [20, 10, 20, 30, 20] $\rightarrow$ [3, 1, 3, 5, 3]

    \item Normalization:
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item MinMax: $\displaystyle \frac{y_i-min}{max-min}$
        \item Standardization: $\displaystyle \frac{y_i-\mu}{\sigma}$
    \end{itemize}

    \item Binarization: 
    \begin{itemize}[topsep=0pt]
        \item Range\ (equal\ width): Depends on variable range \\
        Example: $y\in[-1,1]: [0.2, -0.1, 0.6] \rightarrow [1, 0, 1]$
        \item Frequency\ (equal\ depth): Depends on variable mean \\
        Example: $\bar{y}=25: [10, 40, 30, 20] \rightarrow [0, 1, 1, 0]$
    \end{itemize} 

    \item Confusion Matrix:
    \begin{tblr}{ 
        columns = {1.4em,c},
        column{1,6} = {0.4em},
        rowsep = 1pt,
        hline{3,6} = {1-5}{black}, hline{1,4-5} = {3-5}{black},
        vline{3,6} = {black}, vline{1,4-5} = {3-5}{black},
        cell{4}{4} = {green7}, cell{2}{7} = {green7},
        cell{3,5}{3,5} = {red7}, cell{3}{7} = {red7},
        cell{4}{3,5} = {green9}, cell{4}{7} = {green9},
        cell{3,5}{4} = {red9}, cell{5}{7} = {red9},
    }
        & & \SetCell[c=3]{c} True & & & & B \\
        & & A & B & C & & TP \\
        \SetCell[r=3]{c}\rotatebox[origin=c]{90}{Pred} & A & TA & FA & FA & & TN \\
        & B & FB & TB & FB & & FP \\
        & C & FC & FC & TC & & FN \\
    \end{tblr}

    \item Metrics:
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item Accuracy = $\displaystyle \frac{TP+TN}{total}$
        \item Error rate = $\displaystyle 1-Accuracy=\frac{FP+FN}{total}$
        \item Recall = $\displaystyle \frac{TP}{TP+FN}$ (Sensitivity)
        \item Fallout = $\displaystyle \frac{TN}{TN+FP}$ (Specificity)
        \item Precision = $\displaystyle \frac{TP}{TP+FP}$
        \item F$_1$ = $\displaystyle \frac{TP}{TP+\frac{1}{2}(FP+FN)}$
    \end{itemize}

    \item Error:
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item Sum of Squares Error: $SSE=\sum(Z-\hat{Z})^2$
        \item Maen Squared Error: $MSE=\frac{1}{n}SSE$
        \item Root Maen Squared Error: $RMSE=\sqrt[]{MSE}$
        \item Mean Absolute Error: $MAE=\frac{1}{n}\sum|Z-\hat{Z}|$
    \end{itemize}

    \item Information Gain: $IG(y_{out}|y_{i})=E(y_{out})-E(y_{out}|y_{i})$

    \item Entropy: $E(y)=-\sum P(x_i) \log(P(x_i))$

    \item Decision trees:
    \begin{enumerate}[topsep=0pt, itemsep=0pt]
        \item Choose feature with highest IG.
        \item Split dataset by that feature, create leaves if necessary.
        \item Repeat until unable to proceed.
    \end{enumerate}
    Prune: (Given a twig)
    \begin{enumerate}[topsep=0pt, itemsep=0pt]
        \item Count it's leaves labels. \\ Example: $\#A=5, \ \#B=6$
        \item Remove it's leaves.
        \item Relabel twig as a leaf. \\ Example: $B(6/11), \ \ \#B>\#A$ 
    \end{enumerate}

    \item Vector Norm: \\[2pt]
    $\displaystyle\left\lVert x\right\rVert _p=\sqrt[p]{\sum_{i=1}^n\left\lvert x_i\right\rvert ^p}$ \;\;\;\;\; $\displaystyle\left\lVert x\right\rVert _\infty=max \left\lvert x_i\right\rvert  $

    \newpage
    \item Matrix Multiplication: \\[3pt]
    $\begin{bmatrix}
        \ldots & \ldots & n \\
        \ldots & \ldots & \ldots \\
        m & \ldots & \ldots \\
    \end{bmatrix} \cdot
    \begin{bmatrix}
        \ldots & \ldots & l \\
        \ldots & \ldots & \ldots \\
        n & \ldots & \ldots \\
    \end{bmatrix} =
    \begin{bmatrix}
        \ldots & \ldots & l \\
        \ldots & \ldots & \ldots \\
        m & \ldots & \ldots \\
    \end{bmatrix}$

    \item Gaussian Distribution:
    \begin{itemize}[topsep=0pt]
        \item Variance: $\displaystyle var=\frac{\sum(y_i-\mu)^2}{n(-1)}$
        \item Standard Deviation: $\displaystyle \sigma=\sqrt[]{var}$
        \item $\displaystyle  P(y|\mu,\sigma)=\frac{1}{\sqrt[]{2\pi}\cdot\sigma}\cdot\exp\left(-\frac{1}{2}\left(\frac{y-\mu}{\sigma}\right)^2\right)$
    \end{itemize}

    \item Gaussian Mixture:
    \begin{itemize}[topsep=0pt]
        \item Covariance: $\displaystyle cov(y_1,y_2)=\frac{\sum(y_{1i}-\mu_1)(y_{2i}-\mu_2)}{n(-1)}$
        \item Covariance Matrix: $\Sigma(y_1,y_2)=
        \begin{bmatrix}
            var(y_1) & cov(y_1,y_2) \\
            cov(y_1,y_2) & var(y_2) \\
        \end{bmatrix}$
        \item $\left\lvert\Sigma\right\rvert=
        \begin{vmatrix}
            a & b \\
            c & d \\
        \end{vmatrix}
        = ad-bc$
        \item $\displaystyle P(y|\mu,\Sigma)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}\cdot exp\left(-\frac{1}{2}(y-\mu)^T\Sigma^{-1}(y-\mu)\right)$
    \end{itemize}

    \item Naive Bayes: 
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item MAP: $\displaystyle P(C|x)=\frac{P(C)P(x|C)}{P(x)}$
        \item ML: $\displaystyle P(C|x)=P(x|C)$
    \end{itemize}
    \begin{enumerate}[topsep=0pt, itemsep=0pt]
        \item Calculate probability for each class.
        \item Calculate $P(x|C)$ for each attribute and each class.
        \item TODO
    \end{enumerate}
    
    \item K-Nearest Neighbors:
    \begin{enumerate}[topsep=0pt, itemsep=0pt]
        \item Distances: (for n variables)
        \begin{itemize}[topsep=0pt]
            \item Manhattan: $\displaystyle \sum |y_{1i}-y_{2i}|$
            \item Euclidean: $\displaystyle \sqrt[]{\sum (y_{1i}-y_{2i})^2}$
            \item Cosine: $\displaystyle \frac{\sum y_{1i}\ y_{2i}}
            {\sqrt[]{\sum y_{1i}^2}\ \sqrt[]{\sum y_{2i}^2}}$
            \item Hamming: \#Differences
        \end{itemize}
        \item Choose K nearest neighbors.
        \item Classify using mean if variable is numeric, \\
        or mode if it is categoric.
        \item If weighted, divide by weight.
    \end{enumerate}

    \item Regressions:
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item Linear: $W=(X^TX)^{-1}X^TZ$
        \item Ridge: $W=(X^TX+\lambda I)^{-1}X^TZ$
    \end{itemize}

    \item Perceptron: \\
    $\hat{Z}=a(W^TX)$, \ $a  \leftarrow$ activation function \\
    If $Z \neq \hat{Z} \longrightarrow W^{'}=W+\eta(Z - \hat{Z})X$  

    \item Neural Networks (MLP):
    \begin{itemize}[topsep=0pt]
        \item Forward: $x^{[0]}\rightarrow z^{[1]}=w^{[1]}x^{[0]}+b^{[1]}\rightarrow x^{[1]}=a\left(z^{[1]}\right) \rightarrow \ldots \rightarrow z^{[i]}=w^{[i]}x^{[i-1]}+b^{[i]} \rightarrow x^{[i]}=a\left(z^{[i]}\right) \rightarrow E $
        \item Backward:
        \begin{itemize}[topsep=0pt, itemsep=3pt]
            \item $\displaystyle \delta^{[last]}=\frac{\partial E}{\partial x^{[last]}}\circ \frac{\partial x^{[last]}}{\partial z^{[last]}}$
            \item $\displaystyle \delta^{[i]}=\left(w^{[i+1]}\right)^T\cdot\delta^{[i+1]}\circ\frac{\partial x^{[i]}}{\partial z^{[i]}}$
            \item $\displaystyle w^{[i]'}=w^{[i]}-\eta\frac{\partial E}{\partial w^{[i]}}$ \;\;\;\, $*\ \displaystyle \frac{\partial E}{\partial w^{[i]}}=\delta^{[i]}\cdot \left(x^{[i-1]}\right)^T$
            \item $\displaystyle b^{[i]'}=b^{[i]}-\eta\frac{\partial E}{\partial b^{[i]}}$ \;\;\;\;\;\;\;\  $*\ \displaystyle \frac{\partial E}{\partial b^{[i]}}=\delta^{[i]}$
        \end{itemize}
        \newpage
        \item Derivatives: \\[3pt]
        \begin{tblr}{
            colspec = { X[c,m]X[c,m]X[c,m] },
            hlines = {black},
            vlines = {black},
        }
            Name & {Error \\ function} & $\displaystyle \frac{\partial E}{\partial x^{[i]}}$ \\
            {Squared \\ Error} & $\displaystyle \frac{1}{2}\left(x^{[i]}-t\right)^2$ & $x^{[i]}-t$ \\
            {Cross-\\-entropy} & $\displaystyle -\sum_{i=1}^{n}t_{i}\log\left(x_{i}^{[i]}\right)$ & $\displaystyle -\frac{t}{x^{[i]}}+\frac{1-t}{1-x^{[i]}}$ \\
        \end{tblr}
        \begin{tblr}{
            colspec = { X[c,m]X[c,m]X[c,m] },
            hlines = {black},
            vlines = {black},
        }
            Name & Activation function & $\displaystyle \frac{\partial x^{[i]}}{\partial z^{[i]}}$ \\
            {Sigmoid \\ $\sigma\left(x\right)$} & $\displaystyle \frac{1}{1+e^{-x}}$ & $x^{[i]}\left(1-x^{[i]}\right)$ \\
            {Hyper. tan. \\ $\tanh\left(x\right)$} & $\displaystyle \frac{e^x-e^{-x}}{e^x+e^{-x}}$ & $1-\left(x^{[i]}\right)^2$ \\
            {ReLU \\ $R(x)$} & {$0\mbox{ if }x<0$ \\ $x\mbox{ if }x\geq 0$} & {$0\mbox{ if }x<0$ \\ $1\mbox{ if }x\geq0$} \\
            {Softmax \\ $S(x)$} & $\displaystyle \frac{e^{x_i}}{\sum_{j=1}^{n}e^{x_j}}$ & $x^{[i]}\left(1-x^{[i]}\right)$ \\
        \end{tblr}
        \begin{tblr}{
            colspec = { X[c,m]X[c,m] },
            hlines = {black},
            vlines = {black},
        }
            Function & Derivative \\
            $\displaystyle e^{f(x)}$ & $e^x\times f'(x)$ \\
        \end{tblr} \\[4pt]
        NOTE:
        \begin{itemize}
            \item When cross-entropy and softmax are combined: \\[2pt]
            $\displaystyle \delta^{[last]}=\frac{\partial E}{\partial z^{[last]}}=x^{[last]}-t$
        \end{itemize}
    \end{itemize}

    \item K-Means:
    \begin{enumerate}[topsep=0pt, itemsep=0pt]
        \item Assign each point to a cluster.
        \item Update centroids: centroid$_{new}=$\ mean of cluster points
        \item Repeat until centroids don't change.
    \end{enumerate}

    \item EM:
    \begin{itemize}[topsep=0pt]
        \item Initializaion: Initial mixture parameters
        \item Expectation (E-step): \\[2pt]
        Calculate weights for each datapoint x$_i$ for each cluster c$_k$: 
        $\displaystyle \gamma_{ki}=\frac{\mathcal{N}(x_i|\mu_k,\Sigma_k)\cdot\pi_k}{\sum_{j = 1}^{k}\mathcal{N}(x_{i}|\mu_{j},\Sigma_{j})\cdot\pi_{j}}$
        \item Maximization (M-step): \\[2pt]
        Update parameters for each cluster: (for n observations)
        \begin{itemize}
            \item $\displaystyle N_{k}={\sum_{i=1}^{n}\gamma_{ki}}$
            \item $\displaystyle \mu_{k}={\frac{1}{N_{k}}\sum_{i=1}^{n}\gamma_{ki}\cdot x_{i}}$
            \item $\displaystyle \Sigma_{k} = \frac{1}{N_{k}}\sum_{i=1}^{n}\gamma_{ki} \cdot (x_{i} - \mu_{k}) \cdot (x_{i} - \mu_{k})^T$
            \item $\displaystyle \pi_{k}={\frac{N_{k}}{N}}$
        \end{itemize}
    \end{itemize}

    \item Sillhouette: $\in$ [-1,1] (the closer to 1 the better)
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item For an observation x$_i$:
        \begin{itemize}[topsep=0pt, itemsep=0pt]
            \item $a$ = average distance of x$_i$ to the points in it's cluster
            \item $b$ = average distance of x$_i$ to points in closest cluster
            \item $\displaystyle s_{observation}=\frac{b-a}{max(a,b)}$
        \end{itemize}
        \item For a cluster: \\[2pt]
        Average of the cluster's observations silhouettes
        \item For the solution: \\[2pt]
        Average of the clusters silhouettes
    \end{itemize}

\end{itemize}

\end{document}