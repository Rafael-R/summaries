\documentclass[twocolumn, 10pt]{article}
\usepackage{blindtext}
\usepackage{enumitem}
\usepackage{geometry}
\usepackage{array}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{amsmath}
\geometry{
    a4paper,
    top=10pt,
    right=10pt,
    bottom=10pt,
    left=10pt
}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}

\begin{document}

\begin{itemize}[leftmargin=*, itemsep=0pt]
    \item Boxplot:
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item $Q_{1}=N\times0.25$
        \item $Q_{2}=N\times0.5$
        \item $Q_{3}=N\times0.75$
        \item $IQR=Q_{3}-Q_{1}$
        \item $Bounds=[Q_{1}-1.5 \times IQR, Q_{3}+1.5 \times IQR]$
    \end{itemize}

    \item Pearson = $\displaystyle \frac{\Sigma(y_{1i}-\bar{y}_{1})(y_{2i}-\bar{y}_{2})}{\sqrt[]{\Sigma(y_{1i}-\bar{y}_{1})^2\times(y_{2i}-\bar{y}_{2})^2}}$

    \item Spearman: Assign ranks and apply Pearson formula. \\
    Example: [20, 10, 20, 30, 20] $\rightarrow$ [3, 1, 3, 5, 3]

    \item Information Gain: $IG(y_{out}|y_{i})=E(y_{out})-E(y_{out}|y_{i})$

    \item Entropy: $E(y)=-\sum P(x_i) \log(P(x_i))$
    
    \item Normalization:
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item MinMax: $\displaystyle \frac{y_i-min}{max-min}$
        \item Standardization: $\displaystyle \frac{y_i-\mu}{\sigma}$
    \end{itemize}

    \item Binarization: 
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item Range\ (equal\ width): Depends on variable range \\
        Example: $y\in[-1,1]: [0.2, -0.1, 0.6] \rightarrow [1, 0, 1]$
        \item Frequency\ (equal\ depth): Depends on variable mean \\
        Example: $\bar{y}=25: [10, 40, 30, 20] \rightarrow [0, 1, 1, 0]$
    \end{itemize} 

    \item Confusion Matrix:
    \begin{tabular}{ | M{0.5em} M{0.5em} | M{2em} M{2em} M{2em} | } \hline
        & & & True & \\
        & & A & B & C \\\hline
        \multirow{3}{*}{\rotatebox[origin=c]{90}{Pred}} & A & TA & FA & FA \\
        & B & FP & TB & FB \\
        & C & FC & FC & TC \\\hline
    \end{tabular}
    
    \item Metrics:
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item Accuracy = $\displaystyle \frac{TP+TN}{total}$
        \item Error rate = $\displaystyle 1-Accuracy=\frac{FP+FN}{total}$
        \item Recall = $\displaystyle \frac{TP}{TP+FN}$ (Sensitivity)
        \item Fallout = $\displaystyle \frac{TN}{N}=\frac{TN}{TN+FP}$ (Specificity)
        \item Precision = $\displaystyle \frac{TP}{TP+FP}$
        \item F$_1$ = $\displaystyle \frac{TP}{TP+\frac{1}{2}(FP+FN)}$
    \end{itemize}
    
    \item Error:
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item MSE = $\sum(Z-\hat{Z})^2$
        \item RMSE = $\sqrt[]{MSE}$
        \item MAE = $\sum|Z-\hat{Z}|$
    \end{itemize}

    \item Decision trees:
    \begin{enumerate}[topsep=0pt, itemsep=0pt]
        \item Choose feature with highest IG.
        \item Split dataset by that feature, create leaves if necessary.
        \item Repeat until unable to proceed.
    \end{enumerate}
    Prune: (Given a twig)
    \begin{enumerate}[topsep=0pt, itemsep=0pt]
        \item Count it's leaves labels. \\ Example: $\#A=5, \ \#B=6$
        \item Remove it's leaves.
        \item Relabel twig as a leaf. \\ Example: $B(6/11), \ \ \#B>\#A$ 
    \end{enumerate}

    \item Gaussian Distribution:
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item Mean: $\displaystyle \mu=\frac{1}{n}\sum y_i$
        \item Standard Deviation: $\displaystyle \sigma=\sqrt[]{\frac{\sum(y_i-\mu)^2}{n-1}}$
        \item $\displaystyle  P(y|\mu,\sigma)=\frac{1}{\sqrt[]{2\pi}\cdot\sigma}\cdot\exp\left(-\frac{1}{2}\left(\frac{y-\mu}{\sigma}\right)^2\right)$
    \end{itemize}

    \newpage
    \item Naive Bayes: 
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item MAP: $\displaystyle P(C|x)=\frac{P(C)P(x|C)}{P(x)}$
        \item ML: $\displaystyle P(C|x)=P(x|C)$
    \end{itemize}
    \begin{enumerate}[topsep=0pt, itemsep=0pt]
        \item Calculate probability for each class.
        \item Calculate $P(x|C)$ for each attribute and each class.
        \item TODO
    \end{enumerate}
    
    \item K-Nearest Neighbors:
    \begin{enumerate}[topsep=0pt, itemsep=0pt]
        \item Distances: (for n variables)
        \begin{itemize}[topsep=0pt]
            \item Manhattan: $\displaystyle \sum |y_{1i}-y_{2i}|$
            \item Euclidean: $\displaystyle \sqrt[]{\sum (y_{1i}-y_{2i})^2}$
            \item Cosine: $\displaystyle \frac{\sum y_{1i}\ y_{2i}}
            {\sqrt[]{\sum y_{1i}^2}\ \sqrt[]{\sum y_{2i}^2}}$
            \item Hamming: \#Differences
        \end{itemize}
        \item Choose K nearest neighbors.
        \item Classify using mean if variable is numeric, \\
        or mode if it is categoric.
        \item If weighted, divide by weight.
    \end{enumerate}

    \item Regressions:
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item Linear: $W=(X^TX)^{-1}X^TZ$
        \item Ridge: $W=(X^TX+\lambda I)^{-1}X^TZ$
    \end{itemize}

    \item Perceptron: \\
    $\hat{Z}=a(W^TX)$, \ $a  \leftarrow$ activation function \\
    Se $Z \neq \hat{Z} \longrightarrow W^{'}=W+\eta(Z - \hat{Z})X$  

    \item Neural Networks (MLP):
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item Forward: $x^{[0]}\rightarrow z^{[1]}=w^{[1]}x^{[0]}+b^{[1]}\rightarrow x^{[1]}=a\left(z^{[1]}\right) \rightarrow \ldots \rightarrow z^{[i]}=w^{[i]}x^{[i-1]}+b^{[i]} \rightarrow x^{[i]}=a\left(z^{[i]}\right) \rightarrow E $
        \item Backward:
        \begin{itemize}[topsep=0pt]
            \item $\displaystyle \delta^{[last]}=\frac{\partial E}{\partial x^{[last]}}\circ \frac{\partial x^{[last]}}{\partial z^{[last]}}$
            \item $\displaystyle \delta^{[i]}=\left(\frac{\partial z^{[i+1]}}{\partial x^{[i]}}\right)^T\cdot\delta^{[i+1]}\circ\frac{\partial x^{[i]}}{\partial z^{[i]}}$
            \item $\displaystyle w^{[i]'}=w^{[i]}-\eta\frac{\partial E}{\partial w^{[i]}}$
            \item $\displaystyle \frac{\partial E}{\partial w^{[i]}}=\delta^{[i]}\cdot \left(\frac{\partial z^{[i]}}{\partial w^{[i]}}\right)^T$
            \item $\displaystyle b^{[i]'}=b^{[i]}-\eta\frac{\partial E}{\partial b^{[i]}}$
            \item $\displaystyle \frac{\partial E}{\partial b^{[i]}}=\delta^{[i]}$
            \item
            \begin{tabular}{ M{7.2em} M{7em} M{4em} }
                $\displaystyle \frac{\partial z^{[i+1]}}{\partial x^{[i]}}=w^{[i+1]}$ &
                $\displaystyle \frac{\partial z^{[i]}}{\partial w^{[i]}}=x^{[i-1]}$ &
                $\displaystyle \frac{\partial z^{[i]}}{\partial b^{[i]}}=1$
            \end{tabular}
        \end{itemize}
        {\renewcommand{\arraystretch}{2}
        \begin{tabular}{ | M{7em} | M{8em} | M{7em} | } \hline
            Name & Error function & $\displaystyle \frac{\partial E}{\partial x^{[i]}}$ \\\hline
            Squared Error & $\displaystyle\frac{1}{2}\left(x^{[i]}-t\right)^2$ & $x^{[i]}-t$ \\\hline
            Cross-entropy & $\displaystyle-\sum_{i=1}^{n}t_{i}\log\left(x_{i}^{[i]}\right)$ & $x^{[i]}-t$ \\\hline
        \end{tabular}}
        {\renewcommand{\arraystretch}{2.2}
        \begin{tabular}{ | M{7em} | M{7em} | M{8em} | } \hline
            Name & Activation function & $\displaystyle \frac{\partial x^{[i]}}{\partial z^{[i]}}$ \\\hline
            Sigmoid & $\displaystyle \frac{1}{1+e^{-x}}$ & $\sigma(z^{[i]})(1-\sigma(z^{[i]}))$ \\\hline
            Hyperbolic tangent & $\displaystyle \frac{e^x-e^{-x}}{e^x+e^{-x}}$ & $1-\tanh\left(z^{[i]}\right)^2$ \\\hline
        \end{tabular}}
    \end{itemize}

    \newpage
    \item Gaussian Mixture:
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item Covariance Matrix: $\Sigma=$ 
        \begin{bmatrix}
            TODO & TODO \\
            symmetric & TODO \\
        \end{bmatrix}
        \item $\displaystyle P(y|\mu,\Sigma)=\\\frac{1}{(2\pi)^{d/2}|\Sigma_{k}|^{1/2}}\cdot exp\left(-\frac{1}{2}(y-\mu)^T\Sigma^{-1}(u-\mu)\right)$
    \end{itemize}

    \item K-Means: \\
    TODO

    \item EM: \\
    TODO

\end{itemize}

\end{document}